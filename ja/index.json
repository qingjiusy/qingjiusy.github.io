


[{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/coding/","section":"Codings","summary":"","title":"Codings","type":"coding"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"\r最初に #\r最近、関連するNLPの基礎を学び、これから自分でLLMに関する具体的な応用に取り組もうとしています。次に、RAGを学ぶ中で得た知識をブログで記録しようと思います。\nRAGとは？ #\rRAG（Retrieval-Augmented Generation）とは、情報検索とテキスト生成を組み合わせた技術で、通常は自然言語処理タスクで使用されます。RAGモデルは、情報検索（Retrieval）と生成（Generation）の2つのステージを組み合わせることで、生成されたテキストの内容の正確性と関連性を向上させます。\nこれはGPTが説明したものですが、とても複雑に見えるでしょうか（笑）。\nたとえば、今、あなたが子供にカメラに関する知識を質問したいとします。最初の方法は、その子供がカメラについてゼロから学び、レンズの絞り、焦点距離などを理解し、どのように写真撮影で使うのかを学ぶことです。この方法は、SFT（Supervised Fine-Tuning）です。つまり、我々のデータ（カメラに関する知識）を入力することで、事前学習されたモデル（子供）が学習し、カメラの知識を持つ子供（目的のモデル）を得るというものです。もう一つの方法はRAGです。RAGは、子供がゼロからカメラについて学ぶのは手間がかかると考え、直接カメラに関する本を子供に渡し、「今から質問するから、この本から答えを探して教えて」と言います。したがって、RAGはモデル（子供）を変更するのではなく、入力（本）を増やして、より良い結果を生成するのを助けるという方法です。\nなぜRAGを使うのか？ #\r明らかなように、RAGはLLMを変更せずに良い結果を得ることができ、finetuneにかかる時間や労力を省くことができます。そのため、頻繁に変化する動的データを学習したい場合には、RAGの使用を検討できます。\nRAGでどのようなデータを扱うことができるのか？ #\rRAGはテキスト形式のデータだけでなく、PDFのようなデータも扱うことができ、とても便利です。それだけでなく、関連するモジュールを統合すれば、マルチモーダルな内容も処理できます。今回の学習では、テキストに焦点を当てました。\n文書検索 #\r次に、RAGがどのようにしてテキスト検索を行うかを説明します。具体的な方法としては、インバーテッドインデックス（倒立索引）に基づくものがあります。\nインバーテッドインデックスに基づく方法 #\r普段使っている検索エンジンは、テキスト検索の一過程であり、いくつかのステップが含まれています。\n簡単に言うと、まずテキストを前処理し、統一された形式に変換します。たとえば、句読点やストップワード（Stop Words）を削除し、すべての文字を小文字に変換するなど。そして、テキストのインデックスを構築します。本質的には、インバーテッドインデックス（倒立表）を構築することです。主なロジックは、各単語とその単語がどの文書に位置しているかを記録することです。インバーテッドインデックスInverted Indexを使用することで、単語がどの文書にあるかを見つけることができます。\nこの図でそのロジックがわかります。簡単に言うと、単語がキーで、文書がバリューとなります。\n最後に、テキスト検索が行われ、ユーザーのクエリが前処理されます。次に、クエリ内の各単語の重みを計算し、検索アルゴリズム（TFIDF、BM25）を使用して関連性の高い文書を上位に配置します。\nTFIDF #\rTF-IDF（Term Frequency-Inverse Document Frequency）は、文書内の単語の重要性を測定するための一般的なテキストマイニングおよび情報検索技術です。\n簡単に言うと、TFは文書内の単語の出現頻度であり、IDFは全文書中で単語がどれだけ珍しいかを表しています。\n計算式：\n$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\nここで、𝑡は単語、𝑑は文書、𝐷は文書集合です。\nBM25 #\rTFIDFの改良版で、TFとIDFに加え、文書の長さ（doc_len）も考慮しています。文書が長ければ長いほどスコアが低くなります。長い文書はより多くの単語を含むことができるため、質問への回答がターゲットに対してあまり適切でない可能性が高くなります。これは直感的に理解しやすいです。\n展望 #\r実際の産業において、特に大規模なデータセットでは、Elasticsearchのような専門の文書検索ツールを使用するべきです。\nセマンティック検索 #\rこれはNLPの基礎の一部といえます。最初のword2vecは単語を意味ベクトルに変換するものでした。BERTなどのモデルは文を意味ベクトルに変換することができます。意味ベクトル間の類似度を計算することで、文同士や単語同士の類似度を計算できます。たとえば、「apple company」と「iphone」は、従来のテキスト検索では、これらの単語に共通する単語がないため、類似度がほとんどないとみなされます。しかし、意味的な観点から見ると、両者は非常に関連しているといえます。\nこのタスクは、Massive Text Embedding Benchmark（mteb）と呼ばれます。\nセマンティック検索の前に、テキストセグメンテーションが必要です。モデルの入力には長さの制限があるためです。\n現在のRAGでは、主にテキスト検索とセマンティック検索の併用によって、より高い精度を実現しています。\nマルチパスリコール #\rテキスト検索で精度を向上させるための一般的な方法で、複数のリコールパスを使用して、最終的に候補文書を得ます。簡単に言うと、複数の検索方法を使用して検索し、その結果を加重計算して最終スコアを得るというのがマルチパスリコールです。\nリランキング #\r精度を向上させるためのもう一つの方法です。具体的には、最初に検索された結果に対して、別のより強力なモデル（Rerankモデルと呼ばれる）を使用して再ランキングし、より精確な結果を得ます。\n複雑さと計算リソースの観点から、Rerankモデルは通常、初期の検索モデルよりも複雑です。そのため、Rerankモデルを直接使用するのではなく、粗いランキングの後に精密なランキングを行います。\nたとえば、今、topkの検索結果を求めたいとします。まず、初期検索モデルを使用してtop(k+N)個の結果を検索し、次に精密なランキングを行い、topk個の結果を得ます。\nまとめ #\rRAGはLLMの応用として非常に有望であり、今後、プロジェクトに取り組み、コードに慣れ親しむ予定です。\n","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/coding/llm_learning1/","section":"Codings","summary":"blog","title":"LLM RAGの初歩的な学習","type":"coding"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/","section":"QingJiu観察した世界","summary":"","title":"QingJiu観察した世界","type":"page"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/categories/%E6%8A%80%E6%9C%AF/","section":"Categories","summary":"","title":"技术","type":"categories"},{"content":"","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/categories/%E6%8A%80%E8%A1%93/","section":"Categories","summary":"","title":"技術","type":"categories"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/life/","section":"Lives","summary":"","title":"Lives","type":"life"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/tags/%E6%9D%82%E8%B0%88/","section":"Tags","summary":"","title":"杂谈","type":"tags"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/tags/%E9%9B%91%E8%AB%87/","section":"Tags","summary":"","title":"雑談","type":"tags"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/categories/%E7%94%9F%E6%B4%BB/","section":"Categories","summary":"","title":"生活","type":"categories"},{"content":"\r初め #\rかっこいいブログをいくつか見かけて、自分も試しに作ってみたくなりました。\nこれからは、コーディングで経験した失敗や、日本での旅行記を載せる場所ができそうです。\n写真を撮りたい！\n","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/life/test/","section":"Lives","summary":"blog","title":"突然、ブログを作りたい","type":"life"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/categories/%E6%97%A5%E5%B8%B8/","section":"Categories","summary":"","title":"日常","type":"categories"},{"content":"","externalUrl":null,"permalink":"/ja/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ja/series/","section":"Series","summary":"","title":"Series","type":"series"}]