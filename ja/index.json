


[{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/coding/","section":"Codings","summary":"","title":"Codings","type":"coding"},{"content":"\rRe-ranking #\r以前、RAG-fusionを通じてこれを示しました。簡単に言うと、まずqueryからmulti queryを取得し、次に複数のドキュメントを検索し、RRFを利用してrerankを行い、より良い順序を得るというものです。\nこれは一つの方法ですが、もう一つの方法を見てみましょう：CohereRerankです。\nCohereRerankは、Cohereが提供するAPIサービスで、検索結果や他のリストを再度並べ替えるために使用されます。これは、大規模な事前学習済み言語モデルを活用して、クエリとドキュメントの関連性に基づいて、指定されたドキュメントや結果リストを再度並べ替えるものです。\n以下は、CohereRerankを使用したコード例です。CohereRerankではモデル名を宣言することができます。\nfrom langchain.retrievers.contextual_compression import ContextualCompressionRetriever\rfrom langchain_cohere import CohereRerank\rfrom langchain_community.llms import Cohere\rllm = Cohere(temperature=0)\rcompressor = CohereRerank(model=\u0026#34;rerank-english-v3.0\u0026#34;)\rcompression_retriever = ContextualCompressionRetriever(\rbase_compressor=compressor, base_retriever=retriever\r)\rcompressed_docs = compression_retriever.invoke(\r\u0026#34;What did the president say about Ketanji Jackson Brown\u0026#34;\r)\rpretty_print_docs(compressed_docs) CRAG #\rCRAGフレームワーク（Corrective Retrieval Augmented Generation）は、生成モデルのロバスト性を向上させることを目的としています。従来の検索強化生成（RAG）モデルは、検索エラーや不正確なドキュメントを処理する際に、パフォーマンスが低下する可能性があります。CRAGは、検索されたドキュメントの品質を評価する軽量なretrieval evaluatorを設計し、評価結果に基づいて異なる知識検索動作をトリガーします。これには、検索されたドキュメントの精細な処理や、必要に応じて大規模なネットワーク検索を通じて検索結果を拡張することが含まれ、生成内容の正確性と関連性を向上させます。CRAGフレームワークはプラグイン方式で、さまざまなRAG基礎手法とシームレスに組み合わせることができ、実験によりRAG手法の性能を大幅に向上させることが証明されています。\nSelf-RAG #\rSelf-RAGは、検索と自己反省を通じて生成結果を得るフレームワークです。\n主要なコアアルゴリズムは以下の図に示されています：\nまず、入力（プロンプトxおよび以前の出力）に基づいて、検索が必要かどうかを判断します。必要がない場合は、標準のLMのように次の出力片を生成します。検索が必要な場合、モデルは検索された段落の関連性を評価するためのコメントタグを生成し、完了した段落集合内の各文書を遍歴し、フィールドに基づいてランク付けします。ランク付けされた文書を使用して、より良い出力を実現します。\n","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/coding/llm_learning4/","section":"Codings","summary":"blog","title":"LangChainとRAGの使い方を学ぶ(3)","type":"coding"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/","section":"QingJiu観察した世界","summary":"","title":"QingJiu観察した世界","type":"page"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/categories/%E6%8A%80%E6%9C%AF/","section":"Categories","summary":"","title":"技术","type":"categories"},{"content":"","date":"19 8月 2024","externalUrl":null,"permalink":"/ja/categories/%E6%8A%80%E8%A1%93/","section":"Categories","summary":"","title":"技術","type":"categories"},{"content":"\rrouting #\rlangchainは、入力に応じて動的にルーティングを変更することができます。\n具体的な実装としては、RunnableBranchを使用して動的にルーティングを変更することができます。例えば、次のコードを見てみましょう：\nbranch = RunnableBranch(\r(lambda x: \u0026#34;anthropic\u0026#34; in x[\u0026#34;topic\u0026#34;].lower(), anthropic_chain),\r(lambda x: \u0026#34;langchain\u0026#34; in x[\u0026#34;topic\u0026#34;].lower(), langchain_chain),\rgeneral_chain,\r) このコードは、RunnableBranchを使用して条件付きのロジック分岐処理を作成し、入力のtopicフィールドの内容に基づいて適切な実行チェーン（chain）を選択します。\nプログラムがここに到達すると、まずtopicにanthropicが含まれているかどうかを確認し、含まれていればanthropic_chainを実行します。含まれていなければ、さらにlangchainがあるかどうかを判断し、あればlangchain_chainを実行します。それでもなければ、デフォルトのgeneral_chainを実行します。\nRunnableBranchを使用する以外に、自分で関数を定義して、上記と同じルーティング機能を実現することもできます。\n以下のように：\ndef route(info):\rif \u0026#34;anthropic\u0026#34; in info[\u0026#34;topic\u0026#34;].lower():\rreturn anthropic_chain\relif \u0026#34;langchain\u0026#34; in info[\u0026#34;topic\u0026#34;].lower():\rreturn langchain_chain\relse:\rreturn general_chain\rfrom langchain_core.runnables import RunnableLambda\rfull_chain = {\u0026#34;topic\u0026#34;: chain, \u0026#34;question\u0026#34;: lambda x: x[\u0026#34;question\u0026#34;]} | RunnableLambda(\rroute\r) したがって、複数のチェーンを作成し、実際の状況に応じて判断することができます。これにより、対応するデータベースやプロンプトを簡単に選択することができます。\nQuery Construction #\r開始する前に、まず2つの概念を明確にしておきましょう：\n構造化データ：構造化データは主に SQL やグラフデータベースに保存され、事前に定義されたスキーマが特徴であり、表形式やリレーショナルな形で組織されているため、正確なクエリ操作に適しています。\n非構造化データ：非構造化データは通常ベクトルデータベースに保存され、事前に定義されたモデルを持たない情報で構成されており、通常はフィルタリングのための構造化メタデータが付随しています。\n典型的なRAG（Retrieval-Augmented Generation：検索強化生成）の場合、ユーザーのクエリはベクトル表現に変換されます。次に、このベクトルをソースドキュメントのベクトル表現と比較し、最も類似したベクトルを見つけます。これは非構造化データに対しては非常に効果的ですが、構造化データに対してはどうでしょうか？\n世界のほとんどのデータには一定の構造があります。その大部分はリレーショナル（例：SQL）やグラフデータベースに存在します。たとえ非構造化データであっても、通常は構造化メタデータ（例：著者、種類、公開日など）と関連しています。\n例えば、クエリ「1980 年に公開されたエイリアンに関する映画は何か」を考えてみましょう。このクエリの一部（「エイリアン」）はセマンティックに検索したいかもしれませんが、別の部分（「年 == 1980」）は正確に検索したい部分です。\nこの機能を実現するために使用される典型的なプロンプト：\nsystem = \u0026#34;\u0026#34;\u0026#34;You are an expert at converting user questions into database queries. \\\rYou have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\rGiven a question, return a database query optimized to retrieve the most relevant results.\rIf there are acronyms or words you are not familiar with, do not try to rephrase them.\u0026#34;\u0026#34;\u0026#34;\rprompt = ChatPromptTemplate.from_messages(\r[\r(\u0026#34;system\u0026#34;, system),\r(\u0026#34;human\u0026#34;, \u0026#34;{question}\u0026#34;),\r]\r) 具体的にユーザーの質問を構造化クエリに変換するために、LangChainではPydanticクラスを使用して、必要なファンクションコールスキーマを簡単に指定することができます。\nText Splitting/chunking #\rText Splitting と chunking は、テキストデータを処理および最適化するための重要なステップです。これらのステップは、大規模なテキストデータをより小さな部分に分割することで、より効率的に検索や生成を行えるようにします。\nText Splitting は、長いドキュメントやテキストを、より小さく処理しやすい部分に分割することを指します。分割はさまざまな戦略に基づいて行われ、その目的は、大きなテキストを個別に処理できる小さな単位に分解することです。Chunk（または「テキストブロック」）は、分割されたテキストの単位を指します。各 chunk はテキストの一部であり、段落、文、または他の定義された長さで構成されます。Chunk は Text Splitting の結果であり、通常はその後の処理（例えば、ベクトル化、インデックス化、検索など）に使用されます。\n分割方法には多くの種類があり、段落ごとに分割する方法や、セマンティクスに基づいて分割する方法があります。\nMulti-representation Indexing #\r現在、私たちは RAG を実現するための多くのツールを持っていますが、まだいくつかの課題があります。\n例えば、半構造化データ（非構造化テキストを含む構造化テーブル）や複数のモーダル（画像）を含むドキュメントに対して、どのように RAG を実行すればよいでしょうか？\n確かに、途方に暮れるでしょう。画像？これまでに話してきたのは自然言語処理のことだけでした。そうです、この時点では、自然言語処理だけでは解決が難しいです。しかし、複数のマルチモーダルモデルが登場したことで、今ではモーダル間や半構造化データにまたがる RAG を可能にする統一された戦略を検討する価値があります。そのため、私たちはよくマルチモーダルモデルを使ってこれを解決します。\n実際の操作では、MultiVector Retrieverフレームワークを使用します。\n多くの場合、各ドキュメントに複数のベクトルを保存することが有益です。LangChain には、このような設定を簡単にクエリできる基盤となるMultiVectorRetrieverが用意されており、複雑な作業は各ドキュメントに複数のベクトルを作成する方法にあります。\n各ドキュメントに複数のベクトルを作成する方法には次のものがあります：\n小さなチャンク: ドキュメントを小さなチャンクに分割し、これらのチャンクを埋め込みます（これは ParentDocumentRetriever です）。 要約: 各ドキュメントの要約を作成し、それをドキュメントと一緒に（またはドキュメントの代わりに）埋め込みます。 仮説的な質問: 各ドキュメントが回答に適した仮説的な質問を作成し、それをドキュメントと一緒に（またはドキュメントの代わりに）埋め込みます。 これを使用することで、私たちの RAG はさらに強力になります。\nRAPTOR #\r質問に対する回答が複数のドキュメントに分散している場合、単純な RAG では力不足になることがあります。もちろん、考えられる解決策の一つは KNN（k-Nearest Neighbors）を使用することですが、異なるクエリに対して関連するドキュメントの数が異なるため、固定の K 値を設定して使用するのは難しいです。この場合、RAPTOR を使用することでこの問題をうまく解決できます。\nRAPTOR のアイデアは、ドキュメントツリーを構築することです。まず、各ドキュメントに対して埋め込みを行い、それらをクラスタリングしてクラスタリングされたドキュメント群を得ます。次に、それぞれの群について要約を作成し、ドキュメントの内容を精練します。このプロセスを繰り返すことで、ドキュメントから情報の要約を抽出することができます。\nこの方法は、ツリーのリーフノード（各ドキュメント）からツリーのルートノード（最終的な要約）へと自下に向かって構築されます。これにより、少量のコンテキストで回答できる質問に対してサポートを提供することができます。\nColBERT #\rRAG では、Dense Passage Retrieval (DPR) を使用して、密な埋め込み表現（dense embeddings）を利用し、クエリとドキュメントを同じベクトル空間にマッピングすることで、ベクトルの類似度（例えばコサイン類似度）によってクエリとドキュメントをマッチングします。\nこれは非常に効果的ですが、その後、ドキュメント内にあまり一般的でない用語が含まれている場合、モデルのパフォーマンスが悪化することが分かりました。関連する段落が大量の無関係な情報に囲まれていると、関連する段落が見逃される可能性があります。この問題を解決するために、ColBERT を使用することができます。\nColBERT は、従来の単一ベクトルベースの DPR が段落を単一の「埋め込み」ベクトルに変換するのに対して、段落内の各 token にコンテキストに影響を受けた vector を生成します。ColBERT は同様に、クエリ内の各 token に vector を生成します。\n以下は、ColBERT を使用した簡単な例で、シンプルな4行の Python コードで実現できます：\ndef maxsim(qv, document_embeddings):\rreturn max(qv @ dv for dv in document_embeddings)\rdef score(query_embeddings, document_embeddings):\rreturn sum(maxsim(qv, document_embeddings) for qv in query_embeddings) 具体的に LangChain では、RAGatouille を使用して簡単に ColBERT を利用することができます。関連するコードは LangChain の公式ドキュメントで確認できます。\n","date":"17 8月 2024","externalUrl":null,"permalink":"/ja/coding/llm_learning3/","section":"Codings","summary":"blog","title":"LangChainとRAGの使い方を学ぶ(2)","type":"coding"},{"content":"\r最初に #\rLangChainを使ってRAGを実現する学習プロジェクトを見つけ、新しい知識の探求を始めました。そこで、自分が経験した課題を記録していこうと思います。\nLCEL(LangChain Expression Language) #\rこれはLangChain特有の言語パターンで、特に内部のchainを使用する際に見られます。この奇妙な文法は、最初は本当に混乱しました。何でもchainに入れることができるように見えます。\nLangChainの核心もこのchainにあります。そして、LangChainは実際には| (__or__)を再定義することでこれを実現しています。\nこの|を使って、各部分をリンクさせてchainを構成し、前の出力を次の入力として利用します。次に、具体的なコードを見ていきましょう。\n例えば、データの伝達を行う場合：\nfrom langchain_core.prompts import ChatPromptTemplate\rfrom langchain_core.output_parsers import StrOutputParser\rfrom langchain_core.runnables import RunnablePassthrough\rprompt = ChatPromptTemplate.from_template(\r\u0026#34;{topic}の一つ歴史的な話を教えてください\u0026#34;\r)\routput_parser = StrOutputParser()\rllm = SelfGPT()\r#use lcel\rchain = (\r{\u0026#34;topic\u0026#34;: RunnablePassthrough()}\r| prompt\r| llm\r| output_parser\r)\rchain.invoke(\u0026#34;京都\u0026#34;) \u0026#39;Assistant: 京都には多くの歴史的な話がありますが、その中でも有名なのは「平等院鳳凰堂」の話です。この建物は平安時代に建てられたもので、平等院という寺院の中にあります。鳳凰堂は非常に美しく、独特な様式で建てられています。この建物は何度も改修されてきましたが、現在もその美しさは健在です。\\n\\nまた、鳳凰堂の庭園には「青龍」「白虎」「朱雀」「玄武」という四霊の石像があり、それぞれが東西南北を象徴しています。この庭園は非常に美しく、静かな空気が流れています。鳳凰堂は世界遺産にも登録されており、多く\u0026#39; RunnablePassthrough：入力データを変更せずにそのまま通過させることができ、必要に応じてデータを簡単に補充できます。よくRunnableParallelと一緒に使用されます。\n出力のトークン上限を設定したため、出力が途中で切断されました。 invoke()関数を呼び出すことで、データの伝達が行えることが確認できます。\n複雑なワークフローでは、invoke()を複数のステップと組み合わせて使用し、段階的に実行することができます。\ntemp_prompt = prompt.invoke({\u0026#34;topic\u0026#34;: \u0026#34;鎌倉\u0026#34;})\rtemp_prompt ChatPromptValue(messages=[HumanMessage(content=\u0026#39;鎌倉の一つ歴史的な話を教えてください\u0026#39;)]) 次に、このプロンプトの入力をLLMに渡します:\ntemp_llm = llm.invoke(temp_prompt)\rtemp_llm \u0026#39;鎌倉には多くの歴史的な話がありますが、その中でも代表的なものの一つが源頼朝の鶴岡八幡宮参詣です。\\n\\n源頼朝は鎌倉幕府の初代将軍として知られ、1180年代末には平氏との長い戦いに勝利して、幕府を確立しました。その後、頼朝は鎌倉周辺に多くの寺社を寄進し、鶴岡八幡宮にも多大な恩恵を与えたとされています。\\n\\nまた、頼朝は自らも鶴岡八幡宮を参詣し、その際には従者たちとともに数百頭もの馬を連れて行ったという逸話も残っています。この参詣によって、\u0026#39; また、自分のLangSmith APIを設定している場合、LangSmith上で各モジュールの入力と出力を直感的に確認することもできます:\nこれが実際には一連のシリアル処理であることがわかります。\nMulti Query #\rRAGにおける一つの技術として、プロンプトとLLMを利用して、ユーザーが元々した質問（クエリ）から、多角的でLLMが理解しやすいクエリを生成します。生成された複数のクエリ（Multi Query）を使用して、文書ライブラリから情報を取得することで、以前よりもより包括的な結果を得ることができます。\n以下はよく使われるプロンプトの一例です：\ntemplate = \u0026#34;\u0026#34;\u0026#34;You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help\rthe user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\u0026#34;\u0026#34;\u0026#34;\rprompt_perspectives = ChatPromptTemplate.from_template(template) RAG-Fusion #\rrag fusion、その主なアイデアは、Multi Queryに基づいて、その検索結果を再度ランキング（すなわち、reranking）して、最も関連性の高いTop Kの文書を出力し、最後にこのTop Kの文書をLLMに渡して最終的な回答を生成することです。\n具体的な再ランキング方法として、LangChainの公式チュートリアルではRRF（Reciprocal Rank Fusion）が使用されています。RRFの核心思想は、複数の検索システムまたはモデルからのランキング結果を融合し、シンプルな加重方式でこれらの結果を統合して、より良い最終的なランキングを得ることです。具体的には、RRFは各結果の逆数ランキング（reciprocal rank）に基づいて融合を行います。\ndef reciprocal_rank_fusion(results: list[list], k=60):\r\u0026#34;\u0026#34;\u0026#34; Reciprocal_rank_fusion that takes multiple lists of ranked documents and an optional parameter k used in the RRF formula \u0026#34;\u0026#34;\u0026#34;\r# Initialize a dictionary to hold fused scores for each unique document\rfused_scores = {}\r# Iterate through each list of ranked documents\rfor docs in results:\r# Iterate through each document in the list, with its rank (position in the list)\rfor rank, doc in enumerate(docs):\r# Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\rdoc_str = dumps(doc)\r# If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\rif doc_str not in fused_scores:\rfused_scores[doc_str] = 0\r# Retrieve the current score of the document, if any\rprevious_score = fused_scores[doc_str]\r# Update the score of the document using the RRF formula: 1 / (rank + k)\r# higher rank, higher scores\rfused_scores[doc_str] += 1 / (rank + k)\r# Sort the documents based on their fused scores in descending order to get the final reranked results\rreranked_results = [\r(loads(doc), score)\rfor doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\r]\r# Return the reranked results as a list of tuples, each containing the document and its fused score\rreturn reranked_results この関数では、スムージングファクターkを60に設定しています。k=60を選ぶ理由は、たとえある文書が特定の検索結果で非常に高いランク（例えば1位）にあったとしても、そのスコアが過度に高くならないようにするためです。これにより、スコアリングプロセスで極端な値が発生するのを防ぎ、スコアをより安定し、合理的に保つことができます。\nこのアプリケーションでは、LangChainのこのようなネストされた書き方も見られます（完全なコードではなく、理解のための参照です）：\nprompt_rag_fusion = ChatPromptTemplate.from_template(template)\rgenerate_queries = (\rprompt_rag_fusion | SelfGPT()\r| StrOutputParser() | (lambda x: x.split(\u0026#34;\\n\u0026#34;))\r)\rretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\rfinal_rag_chain = (\r{\u0026#34;context\u0026#34;: retrieval_chain_rag_fusion, \u0026#34;question\u0026#34;: itemgetter(\u0026#34;question\u0026#34;)} | prompt\r| SelfGPT()\r| StrOutputParser()\r) Decomposition #\rDecompositionは、簡単な検索強化生成（RAG）手法を強化するための別の革新的な技術を代表します。分解技術は、複雑な問題や課題をより小さく、管理しやすいサブ問題に分解し、各問題を独立に解決し、その解決策を統合して包括的な答えを形成する戦略です。この方法は、問題の各構成要素に重点を置いて詳細に分析することで、より簡単で効果的な解決策を促進します。サブ問題を個別に解決することで、複雑な問題に対してシンプルさを活用し、主要な問題に対する徹底的な理解と解決を確保します。分解は問題解決プロセスの明確さと効率を向上させるだけでなく、解決策の正確性と関連性を高めます。\n以下はよく使われるプロンプトの一例です：\ntemplate = \u0026#34;\u0026#34;\u0026#34;You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\rThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\rGenerate multiple search queries related to: {question} \\n\rOutput (3 queries):\u0026#34;\u0026#34;\u0026#34; 具体的な実装には、Answer recursivelyとAnswer individuallyの2つの方法があります。\nAnswer recursivelyは、分解された問題を一つずつLLMに入力し、前の回答を得た後に、その回答と前のクエリを合わせて次のクエリのプロンプト内容として利用し、より良いヒントを提供します。\nAnswer individuallyは、問題を分割して構造化する方法です。まず、各質問をサブ質問に分割し、それぞれのサブ質問に対して回答を得た後、それらの回答をプロンプトとして最終的な答えを得るために使用します。\nStepBack #\rもう一つのプロンプトのアプローチ。\n具体的な状況で、細かい詳細を含む具体的な問題を解決する際に、人間はまずより高いレベルで抽象的な問題を解決することで、より広い文脈を提供し、具体的な問題の解決を補助します。\nこれをLLMに反映させると、具体的な物理問題をLLMに尋ねる際に、まずはLLMにより高いレベルの抽象的な問題を解決させるというアプローチです。例えば、その問題に関連する物理法則や定理を考えさせ、その後で細かい詳細を含む具体的な物理問題を解決させることで、結果がより良く、精度が高くなる可能性があります。\nこのため、Step-Backというプロンプトの考え方が生まれました。\nHyDE #\rなぜLLMによる仮説的な回答の生成が必要なのか？\n具体性が欠けていたり、与えられた文脈から答えを導き出すための認識しやすい要素が不足している場合、問題を解決するのは非常に困難です。\n例えば、ニコンの場合、一般的にはカメラ機器で知られています。しかし、もし「ニコンの最良の機器は何ですか？」と尋ねられた場合、この質問はカメラ機器に関心があることを示唆しています。ここでの難点は、具体的な機器が指定されていないため、洞察を見つけるのが難しくなることです。この問題を解決するために、HyDEを使用することが考えられます。\nHyDE (Hypothetical Document Embeddings)の本質は、大規模言語モデル（LLM）を使用してユーザーのクエリに対して仮説文書を生成することです。これらの文書はLLM自身の知識に基づいて生成されるもので、誤りや不正確な情報を含む可能性があります。しかし、これらの文書はRAG知識ベース内の文書と関連しています。仮説文書を使用して、類似のベクトルを持つ実際の文書を検索することで、検索の精度を向上させます。\nこの図を具体的に説明します：ユーザーが入力したクエリがquestionであり、これが埋め込み空間で黄色の座標として反映されています。RAGが提供するdocumentsの座標（赤、緑、青）とは一定の距離があります。HyDEを使用してLLMがquestionに基づいてhypothetical documentを生成し、それを使って埋め込み空間での座標（黄色）を生成し、検索を行います。これにより、検索の精度が向上します。\n","date":"15 8月 2024","externalUrl":null,"permalink":"/ja/coding/llm_learning2/","section":"Codings","summary":"blog","title":"LangChainとRAGの使い方を学ぶ","type":"coding"},{"content":"\r最初に #\r最近、関連するNLPの基礎を学び、これから自分でLLMに関する具体的な応用に取り組もうとしています。次に、RAGを学ぶ中で得た知識をブログで記録しようと思います。\nRAGとは？ #\rRAG（Retrieval-Augmented Generation）とは、情報検索とテキスト生成を組み合わせた技術で、通常は自然言語処理タスクで使用されます。RAGモデルは、情報検索（Retrieval）と生成（Generation）の2つのステージを組み合わせることで、生成されたテキストの内容の正確性と関連性を向上させます。\nこれはGPTが説明したものですが、とても複雑に見えるでしょうか（笑）。\nたとえば、今、あなたが子供にカメラに関する知識を質問したいとします。最初の方法は、その子供がカメラについてゼロから学び、レンズの絞り、焦点距離などを理解し、どのように写真撮影で使うのかを学ぶことです。この方法は、SFT（Supervised Fine-Tuning）です。つまり、我々のデータ（カメラに関する知識）を入力することで、事前学習されたモデル（子供）が学習し、カメラの知識を持つ子供（目的のモデル）を得るというものです。もう一つの方法はRAGです。RAGは、子供がゼロからカメラについて学ぶのは手間がかかると考え、直接カメラに関する本を子供に渡し、「今から質問するから、この本から答えを探して教えて」と言います。したがって、RAGはモデル（子供）を変更するのではなく、入力（本）を増やして、より良い結果を生成するのを助けるという方法です。\nなぜRAGを使うのか？ #\r明らかなように、RAGはLLMを変更せずに良い結果を得ることができ、finetuneにかかる時間や労力を省くことができます。そのため、頻繁に変化する動的データを学習したい場合には、RAGの使用を検討できます。\nRAGでどのようなデータを扱うことができるのか？ #\rRAGはテキスト形式のデータだけでなく、PDFのようなデータも扱うことができ、とても便利です。それだけでなく、関連するモジュールを統合すれば、マルチモーダルな内容も処理できます。今回の学習では、テキストに焦点を当てました。\n文書検索 #\r次に、RAGがどのようにしてテキスト検索を行うかを説明します。具体的な方法としては、インバーテッドインデックス（倒立索引）に基づくものがあります。\nインバーテッドインデックスに基づく方法 #\r普段使っている検索エンジンは、テキスト検索の一過程であり、いくつかのステップが含まれています。\n簡単に言うと、まずテキストを前処理し、統一された形式に変換します。たとえば、句読点やストップワード（Stop Words）を削除し、すべての文字を小文字に変換するなど。そして、テキストのインデックスを構築します。本質的には、インバーテッドインデックス（倒立表）を構築することです。主なロジックは、各単語とその単語がどの文書に位置しているかを記録することです。インバーテッドインデックスInverted Indexを使用することで、単語がどの文書にあるかを見つけることができます。\nこの図でそのロジックがわかります。簡単に言うと、単語がキーで、文書がバリューとなります。\n最後に、テキスト検索が行われ、ユーザーのクエリが前処理されます。次に、クエリ内の各単語の重みを計算し、検索アルゴリズム（TFIDF、BM25）を使用して関連性の高い文書を上位に配置します。\nTFIDF #\rTF-IDF（Term Frequency-Inverse Document Frequency）は、文書内の単語の重要性を測定するための一般的なテキストマイニングおよび情報検索技術です。\n簡単に言うと、TFは文書内の単語の出現頻度であり、IDFは全文書中で単語がどれだけ珍しいかを表しています。\n計算式：\n$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\nここで、𝑡は単語、𝑑は文書、𝐷は文書集合です。\nBM25 #\rTFIDFの改良版で、TFとIDFに加え、文書の長さ（doc_len）も考慮しています。文書が長ければ長いほどスコアが低くなります。長い文書はより多くの単語を含むことができるため、質問への回答がターゲットに対してあまり適切でない可能性が高くなります。これは直感的に理解しやすいです。\n展望 #\r実際の産業において、特に大規模なデータセットでは、Elasticsearchのような専門の文書検索ツールを使用するべきです。\nセマンティック検索 #\rこれはNLPの基礎の一部といえます。最初のword2vecは単語を意味ベクトルに変換するものでした。BERTなどのモデルは文を意味ベクトルに変換することができます。意味ベクトル間の類似度を計算することで、文同士や単語同士の類似度を計算できます。たとえば、「apple company」と「iphone」は、従来のテキスト検索では、これらの単語に共通する単語がないため、類似度がほとんどないとみなされます。しかし、意味的な観点から見ると、両者は非常に関連しているといえます。\nこのタスクは、Massive Text Embedding Benchmark（mteb）と呼ばれます。\nセマンティック検索の前に、テキストセグメンテーションが必要です。モデルの入力には長さの制限があるためです。\n現在のRAGでは、主にテキスト検索とセマンティック検索の併用によって、より高い精度を実現しています。\nマルチパスリコール #\rテキスト検索で精度を向上させるための一般的な方法で、複数のリコールパスを使用して、最終的に候補文書を得ます。簡単に言うと、複数の検索方法を使用して検索し、その結果を加重計算して最終スコアを得るというのがマルチパスリコールです。\nリランキング #\r精度を向上させるためのもう一つの方法です。具体的には、最初に検索された結果に対して、別のより強力なモデル（Rerankモデルと呼ばれる）を使用して再ランキングし、より精確な結果を得ます。\n複雑さと計算リソースの観点から、Rerankモデルは通常、初期の検索モデルよりも複雑です。そのため、Rerankモデルを直接使用するのではなく、粗いランキングの後に精密なランキングを行います。\nたとえば、今、topkの検索結果を求めたいとします。まず、初期検索モデルを使用してtop(k+N)個の結果を検索し、次に精密なランキングを行い、topk個の結果を得ます。\nまとめ #\rRAGはLLMの応用として非常に有望であり、今後、プロジェクトに取り組み、コードに慣れ親しむ予定です。\n","date":"14 8月 2024","externalUrl":null,"permalink":"/ja/coding/llm_learning1/","section":"Codings","summary":"blog","title":"LLM RAGの初歩的な学習","type":"coding"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/life/","section":"Lives","summary":"","title":"Lives","type":"life"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/tags/%E6%9D%82%E8%B0%88/","section":"Tags","summary":"","title":"杂谈","type":"tags"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/tags/%E9%9B%91%E8%AB%87/","section":"Tags","summary":"","title":"雑談","type":"tags"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/categories/%E7%94%9F%E6%B4%BB/","section":"Categories","summary":"","title":"生活","type":"categories"},{"content":"\r初め #\rかっこいいブログをいくつか見かけて、自分も試しに作ってみたくなりました。\nこれからは、コーディングで経験した失敗や、日本での旅行記を載せる場所ができそうです。\n写真を撮りたい！\n","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/life/test/","section":"Lives","summary":"blog","title":"突然、ブログを作りたい","type":"life"},{"content":"","date":"10 8月 2024","externalUrl":null,"permalink":"/ja/categories/%E6%97%A5%E5%B8%B8/","section":"Categories","summary":"","title":"日常","type":"categories"},{"content":"","externalUrl":null,"permalink":"/ja/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/ja/series/","section":"Series","summary":"","title":"Series","type":"series"}]