


[{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/coding/","section":"Codings","summary":"","title":"Codings","type":"coding"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"\r最开始 #\r最近学习了相关的NLP的基础，打算开始着手自己搞LLM相关的具体应用了。接下来就用blog记录一下自己学RAG中学到的东西。\n什么是RAG？ #\rRAG(Retrieval-Augmented Generation)是一种将信息检索和文本生成结合的技术，通常用于自然语言处理任务中。RAG模型将信息检索（Retrieval）和生成 (Generation)两个阶段结合起来，以提高生成的文本内容的准确性和相关性。\n以上是GPT说的，看起来非常的高大上是不是(笑)。\n让我来打个比方：现在有一个小孩，你想问它有关于相机方面的知识。第一种方案是让小孩开始从零学习相机方面的知识，自己去理解镜头中的光圈，焦段\u0026hellip;该怎 么去摄影的时候使用，这种方案就是SFT(Supervised Fine-Tuning),通过输入我们的数据(相机方面的知识)去让预训练模型(小孩)学习，从而得到我们想要的模型(懂得相机知识的小孩)。而另外一种方法就是RAG，它觉得让小孩从零学习相机方面的知识还是太麻烦了，直接找了一本相机的书给小孩，和他说：“现在我要问你问题,你从这本书里面找答案给我结果。”所以RAG这个方法并不会改变我们的模型(小孩)，而是增加了输入(书)来辅助模型生成更好的结果。\n为什么要用RAG？ #\r显而易见的，RAG可以在不改变LLM的情况下去获得好的结果，省去了finetune时耗费的时间和经历。因此，当我们想要学习一个变化很频繁的动态数据时，可以考虑使用RAG。\nRAG可以处理哪些数据？ #\rRAG不仅可以处理文本类型的数据，也可以处理PDF这种类型的数据，可谓是非常方便了。不仅如此，它还可以处理多模态相关的内容，只要集成了相关的模块即可。在这次学习中，我着目于文字相关内容。\n文件检索 #\r接下来讲讲RAG怎么进行文本检索。具体方法有基于倒排索引。\n基于倒排索引 #\r我们平时在使用的搜索引擎就是一个文本检索的过程，它是包含多个步骤的。\n简单来说，首先就是对文本进行预处理，变成规范统一的格式。比如去除标点，停用词Stop Words，字母全部变成小写等等\u0026hellip;然后就是构建文本的索引，本质就是构建倒排表。主要逻辑是记录每个词项，以及词项所在文档中间的位置信息。通过倒排表Inverted Index我们可以找到词所在的文档。\n可以在这张图片中看到这个逻辑。简单来说，词作为key，文档作为value。\n最后就是文本检索，预处理用户查询，然后计算查询中每个词项的权重。并利用检索算法(TFIDF,BM25)进行相关排序，把相关性高的文档排到前面。\nTFIDF #\rTF-IDF（Term Frequency-Inverse Document Frequency）是一种常用的文本挖掘和信息检索技术，用于衡量词汇在文档中的重要性。\n简单来说TF是词项在文档中的词频，表示一个词在文档中出现的频率。IDF是逆文档频率，表示一个词在所有文档中出现的稀有程度。\n计算公式：\n$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n其中，𝑡是词，𝑑是文档，𝐷是文档集合。\nBM25 #\r对TFIDF的一种改进，不仅考虑到了TF，IDF。还考虑到了文档的长度doc_len。文档越长，分数越小。因为越长的文档能包含的词项肯定越多越杂，所以对问题的针对性回答肯定不会那么好，直观上比较容易理解。\n展望 #\r在实际工业中，特别是大规模数据集中，应该使用专业的文档检索工具，比如Elasticsearch。\n语义检索 #\r这一块算是NLP基础。最早的word2vec就是将词变成语义向量。BERT等模型则可以做到把句子变成语义向量。通过计算语义向量之间的相似度，可以计算出句子间，词之间的相似程度。比如apple company和iphone，如果只靠之前的文本检索，由于这两个词之间都没有相同的词，可以说是一点相似度都没有。但是我们如果从语义的角度出发，可以说两者之间有很大的关系。\n这个任务叫做mteb：Massive Text Embedding Benchmark。\n在语义检索之前，需要对文本进行文本切分，因为模型的输入是有长度上限的。\n目前在RAG中，主要是文本检索和语义检索结合使用，这样可以得到一个更高的精度。\n多路召回 #\r文本检索中增加检索精度的常用方法，通过多种召回路径进行综合，最后得到候选文档。简单来说就是通过多种检索方法进行检索，然后对结果进行加权计算得到最终的分数，这就是多路召回。\n重排序 #\r另一种增加精度的方法。具体来说就是对第一次检索出的结果，用其他更强大的模型(也叫Rerank模型)进行重排序，得到更加精确的结果。\n从复杂性和计算资源的角度来看，Rerank模型通常比初始检索模型更复杂。所以不直接使用Rerank模型，而是在粗排之后进行精排。\n举个例子，比如现在想要topk的检索结果。先用初始检索模型检索出top(k+N)个结果，然后用精排对k+N个文档重新排序，得到topk个结果。\n总结 #\rRAG作为LLM的一项应用，还是蛮有前景的，之后打算把项目摸一遍，熟悉熟悉代码。\n","date":"14 August 2024","externalUrl":null,"permalink":"/coding/llm_learning1/","section":"Codings","summary":"blog","title":"LLM RAG初步学习","type":"coding"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/","section":"QingJiu","summary":"","title":"QingJiu","type":"page"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/categories/%E6%8A%80%E6%9C%AF/","section":"Categories","summary":"","title":"技术","type":"categories"},{"content":"","date":"14 August 2024","externalUrl":null,"permalink":"/ja/categories/%E6%8A%80%E8%A1%93/","section":"Categories","summary":"","title":"技術","type":"categories"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/life/","section":"Lives","summary":"","title":"Lives","type":"life"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/ja/categories/%E6%97%A5%E5%B8%B8/","section":"Categories","summary":"","title":"日常","type":"categories"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/categories/%E7%94%9F%E6%B4%BB/","section":"Categories","summary":"","title":"生活","type":"categories"},{"content":"\r最开始 #\r看到了一些很帅气的blog，于是自己也想搭个玩玩\n以后就有地方写coding上踩过的一些坑，以及上传自己在日本的游记了。\n想要拍照！\n","date":"10 August 2024","externalUrl":null,"permalink":"/life/test/","section":"Lives","summary":"blog","title":"突如其来地想搭建一个blog","type":"life"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/tags/%E6%9D%82%E8%B0%88/","section":"Tags","summary":"","title":"杂谈","type":"tags"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/ja/tags/%E9%9B%91%E8%AB%87/","section":"Tags","summary":"","title":"雑談","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]