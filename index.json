


[{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/coding/","section":"Codings","summary":"","title":"Codings","type":"coding"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/","section":"QingJiu","summary":"","title":"QingJiu","type":"page"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/categories/%E6%8A%80%E6%9C%AF/","section":"Categories","summary":"","title":"技术","type":"categories"},{"content":"","date":"17 August 2024","externalUrl":null,"permalink":"/ja/categories/%E6%8A%80%E8%A1%93/","section":"Categories","summary":"","title":"技術","type":"categories"},{"content":"\rrouting #\rlangchain可以根据输入动态更改路由。\n具体实现上可以使用RunnableBranch来动态更改路由，比如我们可以来看看这一段代码：\nbranch = RunnableBranch(\r(lambda x: \u0026#34;anthropic\u0026#34; in x[\u0026#34;topic\u0026#34;].lower(), anthropic_chain),\r(lambda x: \u0026#34;langchain\u0026#34; in x[\u0026#34;topic\u0026#34;].lower(), langchain_chain),\rgeneral_chain,\r) 这段代码使用RunnableBranch创建了一个条件逻辑分支处理器，该处理器根据输入的topic字段内容选择适当的执行链（chain）。\n当程序执行到这里，他会先检查topic中是否有anthropic，如果有则去执行anthropic_chain。如果没有，进一步判断是否有langchain，同样的，如果有则去执行langchain_chain。如果还是没有则会执行默认general_chain。\n除了使用RunnableBranch，也可以去自己定义函数完成与上面一样的路由功能。\n如下所示：\ndef route(info):\rif \u0026#34;anthropic\u0026#34; in info[\u0026#34;topic\u0026#34;].lower():\rreturn anthropic_chain\relif \u0026#34;langchain\u0026#34; in info[\u0026#34;topic\u0026#34;].lower():\rreturn langchain_chain\relse:\rreturn general_chain\rfrom langchain_core.runnables import RunnableLambda\rfull_chain = {\u0026#34;topic\u0026#34;: chain, \u0026#34;question\u0026#34;: lambda x: x[\u0026#34;question\u0026#34;]} | RunnableLambda(\rroute\r) 因此，我们可以创建多根chain，在实际中根据情况判断。这样可以很方便的选择对应的database或者prompt。\nQuery Construction #\r在开始之前，先明确两个概念：\n结构化数据：结构化数据主要存储在 SQL 或图形数据库中，其特点是预定义的模式，并以表格或关系的形式组织，使其适用于精确的查询操作。\n非结构化数据：非结构化数据通常存储在向量数据库中，由没有预定义模型的信息组成，通常伴有用于过滤的结构化元数据。\n对于典型的检索增强生成 (RAG)，用户查询会转换为向量表示。然后将此向量与源文档的向量表示进行比较，以找到最相似的向量。这对于非结构化数据来说效果很好，但结构化数据呢？\n世界上大多数数据都有一定的结构。其中大部分数据存在于关系（例如，SQL）或图形数据库中。即使是非结构化数据通常也与结构化元数据相关联（例如，作者、类型、发布日期等）。\n例如，考虑查询1980 年有哪些关于外星人的电影。有一部分（外星人）我们可能希望从语义上查找，但也有一个组件（“年份 == 1980”）我们希望以确切的方式查找。\n实现这个功能常用的prompt：\nsystem = \u0026#34;\u0026#34;\u0026#34;You are an expert at converting user questions into database queries. \\\rYou have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\rGiven a question, return a database query optimized to retrieve the most relevant results.\rIf there are acronyms or words you are not familiar with, do not try to rephrase them.\u0026#34;\u0026#34;\u0026#34;\rprompt = ChatPromptTemplate.from_messages(\r[\r(\u0026#34;system\u0026#34;, system),\r(\u0026#34;human\u0026#34;, \u0026#34;{question}\u0026#34;),\r]\r) 而具体为了将用户问题转换为结构化查询，使用LangChain可以通过Pydantic类轻松指定所需的function call schema。\nText Splitting/chunking #\rText Splitting 和 chunking 是处理和优化文本数据的重要步骤。这些步骤帮助将大规模文本数据拆分成更小的片段，以便更高效地进行检索和生成。\nText Splitting 是指将长文档或文本分成更小的、易于处理的部分。拆分可以根据多种策略进行，目的是将大块文本分解为可以单独处理的更小单元。Chunk（或称为“文本块”）是指经过拆分后的文本单元。每个 chunk 是文本的一部分，可以是一个段落、句子或其他定义的长度。Chunk 是 Text Splitting 过程的结果，通常用来进行后续的处理，如向量化、索引和检索。\n有很多Splitting的方法，可以根据段落split，也可以根据语义来。\nMulti-representation Indexing #\r目前我们已经有了很多的工具去实现我们的rag，但是仍然有一些不足。\n试想一下，对于包含半结构化数据（带有非结构化文本的结构化表格）和多种模态（图像）的文档上该怎么进行rag呢？\n你肯定会觉得无从下手，图像？明明我们之前讲的都是自然语言处理。是的，在这个时候光是自然语言处理很难解决它。而随着多个多模态模型的出现，现在值得考虑统一的策略以跨模态和半结构化数据启用 RAG。因此我们常常用多模态模型来解决它。\n而在实际操作中，使用了MultiVector Retriever框架。\n每个文档存储多个向量通常是有益的。在许多用例中，这是有益的。 LangChain 有一个基础 MultiVectorRetriever，可以轻松查询此类设置。很多复杂性在于如何为每个文档创建多个向量。\n为每个文档创建多个向量的方法包括：\n较小的块：将文档分割成较小的块，然后嵌入这些块（这是 ParentDocumentRetriever）。 摘要：为每个文档创建摘要，将其与文档一起嵌入（或代替文档）。 假设性问题：创建每个文档都适合回答的假设性问题，将这些问题与文档一起嵌入（或代替文档）。\n使用它可以让我们的rag更加强大。\nRAPTOR #\r当我们的问题所需要的回答可以分布在多个文档中时，简单的rag就有点力不从心了。当然，我们能够想到的一种解决方法是去使用KNN，但是不同的查询所对应的文档数量肯定不同，所以固定一个K来使用就比较困难了。这时候，我们使用RAPTOR可以很好的解决这个问题。\nRAPTOR的想法是构建一棵文档树，如上图所示。首先对各个文档进行embedding，然后对它们进行clustering，得到聚类后的文档群，再各自进行summary，精炼文档内容。持续重复这一过程，就能够提取出这些文档中的信息摘要。\n整个方法就是自下而上，从树的leaf节点（各个文档）构建到树的根节点（最后的summary）。这样可以为那些只需要少量上下文就能回答的问题提供支持。\nColBERT #\r在RAG中，我们使用Dense Passage Retrieval (DPR)，使用一种密集的嵌入表示（dense embeddings），将查询和文档映射到相同的向量空间中，从而可以通过向量相似度（如余弦相似度）来匹配查询和文档。\n这十分有效，但是我们之后发现，当文档中存在一些不常见的术语的时候，模型的效果不好。如果相关段落被大量不相关信息包围，可能会导致相关段落被遗漏。而为了解决这一困境，可以使用ColBERT。\nColBERT 不是传统的基于单向量的 DPR 将段落转换为单个“嵌入”向量，而是为段落中的每个token生成一个受上下文影响的vector。 ColBERT 类似地为查询中的每个token生成vector。\n以下是一个简单的使用ColBERT的实例，用简单四行python代码即可实现：\ndef maxsim(qv, document_embeddings):\rreturn max(qv @ dv for dv in document_embeddings)\rdef score(query_embeddings, document_embeddings):\rreturn sum(maxsim(qv, document_embeddings) for qv in query_embeddings) 而具体在langchain中，我们可以使用RAGatouille来简单的使用ColBERT。相关代码可以去langchain官方文档查看。\n","date":"17 August 2024","externalUrl":null,"permalink":"/coding/llm_learning3/","section":"Codings","summary":"blog","title":"学习使用langchain和RAG(2)","type":"coding"},{"content":"\r最开始 #\r发现了一个用langchain去做RAG的学习项目，于是就开始新知识的探索，记录一下自己踩过的坑\nLCEL(LangChain Expression Language) #\r这是langchain特有的一种语言模式，尤其是在使用里面的chain的时候。这种奇怪的语法真的在一开始给我看懵了，貌似什么东西都能往chain里面塞的。\n而langchain的核心也就在这个chain上。而Langchain实际上是通过重写了| (__or__) 来实现它。\n而通过|，可以将各个部分链接在一起组成一条chain，前面的输出作为后面的输入。接下来通过代码具体看看。\n比如进行数据传递：\nfrom langchain_core.prompts import ChatPromptTemplate\rfrom langchain_core.output_parsers import StrOutputParser\rfrom langchain_core.runnables import RunnablePassthrough\rprompt = ChatPromptTemplate.from_template(\r\u0026#34;{topic}の一つ歴史的な話を教えてください\u0026#34;\r)\routput_parser = StrOutputParser()\rllm = SelfGPT()\r#use lcel\rchain = (\r{\u0026#34;topic\u0026#34;: RunnablePassthrough()}\r| prompt\r| llm\r| output_parser\r)\rchain.invoke(\u0026#34;京都\u0026#34;) \u0026#39;Assistant: 京都には多くの歴史的な話がありますが、その中でも有名なのは「平等院鳳凰堂」の話です。この建物は平安時代に建てられたもので、平等院という寺院の中にあります。鳳凰堂は非常に美しく、独特な様式で建てられています。この建物は何度も改修されてきましたが、現在もその美しさは健在です。\\n\\nまた、鳳凰堂の庭園には「青龍」「白虎」「朱雀」「玄武」という四霊の石像があり、それぞれが東西南北を象徴しています。この庭園は非常に美しく、静かな空気が流れています。鳳凰堂は世界遺産にも登録されており、多く\u0026#39; RunnablePassthrough:允许将输入数据传递而不做更改，可以在需要时很方便地进行数据的填充。经常会和RunnableParallel一起使用。\n由于我设置了输出的token上限，于是输出就截断了。 可以看到通过调用invoke()函数可以进行数据的传递。\n而在复杂的工作流中，invoke()可以与多个步骤结合使用，进行逐步执行。\ntemp_prompt = prompt.invoke({\u0026#34;topic\u0026#34;: \u0026#34;鎌倉\u0026#34;})\rtemp_prompt ChatPromptValue(messages=[HumanMessage(content=\u0026#39;鎌倉の一つ歴史的な話を教えてください\u0026#39;)]) 再将这个prompt的输入传入llm：\ntemp_llm = llm.invoke(temp_prompt)\rtemp_llm \u0026#39;鎌倉には多くの歴史的な話がありますが、その中でも代表的なものの一つが源頼朝の鶴岡八幡宮参詣です。\\n\\n源頼朝は鎌倉幕府の初代将軍として知られ、1180年代末には平氏との長い戦いに勝利して、幕府を確立しました。その後、頼朝は鎌倉周辺に多くの寺社を寄進し、鶴岡八幡宮にも多大な恩恵を与えたとされています。\\n\\nまた、頼朝は自らも鶴岡八幡宮を参詣し、その際には従者たちとともに数百頭もの馬を連れて行ったという逸話も残っています。この参詣によって、\u0026#39; 同时，如果我们设置了自己的langsmith api，也可以去langsmith上直观的查看各个模块的输入输出：\n可以看到这其实是一系列的串行处理。\nMulti Query #\r一种RAG中的技术，利用prompt和llm从原本用户提问的query中生成多角度的，适合llm去理解的query。通过生成的Multi Query，再对文档库进行retrieve，可以比之前得到更加全面的结果。\n以下是一段常用的prompt：\ntemplate = \u0026#34;\u0026#34;\u0026#34;You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help\rthe user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\u0026#34;\u0026#34;\u0026#34;\rprompt_perspectives = ChatPromptTemplate.from_template(template) RAG-Fusion #\rrag fusion，它的主要思想是在Multi Query的基础上，对其检索结果进行重新排序(即reranking)后输出Top K个最相关文档，最后将这top k个文档喂给LLM并生成最终的答案(answer)。\n具体的重新排序方法，在langchain的官方教学文档中采用了RRF(Reciprocal rank fusion)。核心思想是将来自多个检索系统或模型的排名结果进行融合，通过一种简单的加权方式来综合这些结果，从而得到一个更好的最终排序。具体来说，RRF 基于每个结果的倒数排名（reciprocal rank）来进行融合。\ndef reciprocal_rank_fusion(results: list[list], k=60):\r\u0026#34;\u0026#34;\u0026#34; Reciprocal_rank_fusion that takes multiple lists of ranked documents and an optional parameter k used in the RRF formula \u0026#34;\u0026#34;\u0026#34;\r# Initialize a dictionary to hold fused scores for each unique document\rfused_scores = {}\r# Iterate through each list of ranked documents\rfor docs in results:\r# Iterate through each document in the list, with its rank (position in the list)\rfor rank, doc in enumerate(docs):\r# Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\rdoc_str = dumps(doc)\r# If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\rif doc_str not in fused_scores:\rfused_scores[doc_str] = 0\r# Retrieve the current score of the document, if any\rprevious_score = fused_scores[doc_str]\r# Update the score of the document using the RRF formula: 1 / (rank + k)\r# higher rank, higher scores\rfused_scores[doc_str] += 1 / (rank + k)\r# Sort the documents based on their fused scores in descending order to get the final reranked results\rreranked_results = [\r(loads(doc), score)\rfor doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\r]\r# Return the reranked results as a list of tuples, each containing the document and its fused score\rreturn reranked_results 在这个函数中，让平滑因子k为60。选择 k=60 是为了确保即使某些文档在某个检索结果中的排名非常靠前（例如排名为 1），其评分也不会过高。这有助于避免在评分过程中出现极端值，使得评分更加稳定和合理。\n在这个应用中也看到了langchain这种嵌套式的写法(不完整的代码，只是为了理解观看)：\nprompt_rag_fusion = ChatPromptTemplate.from_template(template)\rgenerate_queries = (\rprompt_rag_fusion | SelfGPT()\r| StrOutputParser() | (lambda x: x.split(\u0026#34;\\n\u0026#34;))\r)\rretrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\rfinal_rag_chain = (\r{\u0026#34;context\u0026#34;: retrieval_chain_rag_fusion, \u0026#34;question\u0026#34;: itemgetter(\u0026#34;question\u0026#34;)} | prompt\r| SelfGPT()\r| StrOutputParser()\r) Decomposition #\r分解Decomposition代表了增强简单检索增强生成（RAG）方法的另一种创新技术。分解技术是一种解决问题的策略，涉及将复杂的问题或问题分解为更小、更易于管理的子问题，独立地解决每个问题，然后整合它们的解决方案以形成全面的答案。这种方法可以对问题的每个组成部分进行重点和详细的分析，从而促进更简单、更有效的解决方案。通过单独解决子问题，它可以利用简单性来解决复杂问题，确保对主要问题的彻底理解和解决。分解不仅增强了问题解决过程的清晰度和效率，而且提高了解决方案的准确性和相关性。\n以下是一段常用的prompt：\ntemplate = \u0026#34;\u0026#34;\u0026#34;You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\rThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\rGenerate multiple search queries related to: {question} \\n\rOutput (3 queries):\u0026#34;\u0026#34;\u0026#34; 在具体实现中，有Answer recursively和Answer individually两种方法。\nAnswer recursively是将分解后的问题一一输入llm，前面answer得到的答案与前面的query一起作为下一个query的prompt内容，来进行更好的提示。\nAnswer individually是一个分总的结构。先把每个question分割成subquestion，分别得到各自的问题，然后对各自的问题进行获得答案。最后把这些问题答案对作为prompt输入，得到最终的答案。\nStepBack #\r另一种prompt思路。\n在具体生活中，人类在解决一个包含很多细节的具体问题时，先站在更高的层次上解决一些更加抽象的问题，可以拓展一个更宽阔的上下文环境，从而辅助解决这个具体的问题。\n反应到LLM中，就是当问LLM一个具体的物理题目时，先让LLM解决一个更加高层次的抽象问题。比如思考这个问题背后用得到物理定律或法则是什么，然后再让LLM去解决那个包含了很多细节的具体的物理题目，可能效果就会更好，准确率更高。\n因此，也就有了Step-Back的prompting思路。\nHyDE #\r那么为什么需要LLM生成假设性回答？\n在面对缺乏具体性或缺乏易于识别的元素从给定上下文中推导答案的问题时，有时候会相当具有挑战性。\n例如，考虑尼康的情况，它通常以摄影器材而闻名。然而，如果有人询问尼康最好的设备是什么，这个问题暗示了对摄影设备的关注。这里的难点在于没有指定具体的设备。因此，寻找洞察力变得有困难。为了解决这个问题，我们可以考虑使用HyDE。\nHyDE (Hypothetical Document Embeddings) 的本质是使用大语言模型 (LLM) 为用户查询生成假设文档。这些文档是根据 LLM 本身的知识生成的，可能包含错误或不准确之处。但是，它们与 RAG 知识库中的文档相关联。然后，通过使用这些假设文档来检索具有相似向量的真实文档，从而提高检索的准确性。\n具体来解释一下这张图：用户本身输入的查询为question，它所反映到嵌入空间中是黄色的坐标，而距离rag提供的documents的坐标（红色，绿色，蓝色）都有一定的距离。而我们使用HyDE去让LLM根据question生成hypothetical document，用它去生成嵌入空间中的坐标（黄色），然后进行检索。这样可以提高检索的准确性。\n","date":"15 August 2024","externalUrl":null,"permalink":"/coding/llm_learning2/","section":"Codings","summary":"blog","title":"学习使用langchain和RAG","type":"coding"},{"content":"\r最开始 #\r最近学习了相关的NLP的基础，打算开始着手自己搞LLM相关的具体应用了。接下来就用blog记录一下自己学RAG中学到的东西。\n什么是RAG？ #\rRAG(Retrieval-Augmented Generation)是一种将信息检索和文本生成结合的技术，通常用于自然语言处理任务中。RAG模型将信息检索（Retrieval）和生成 (Generation)两个阶段结合起来，以提高生成的文本内容的准确性和相关性。\n以上是GPT说的，看起来非常的高大上是不是(笑)。\n让我来打个比方：现在有一个小孩，你想问它有关于相机方面的知识。第一种方案是让小孩开始从零学习相机方面的知识，自己去理解镜头中的光圈，焦段\u0026hellip;该怎 么去摄影的时候使用，这种方案就是SFT(Supervised Fine-Tuning),通过输入我们的数据(相机方面的知识)去让预训练模型(小孩)学习，从而得到我们想要的模型(懂得相机知识的小孩)。而另外一种方法就是RAG，它觉得让小孩从零学习相机方面的知识还是太麻烦了，直接找了一本相机的书给小孩，和他说：“现在我要问你问题,你从这本书里面找答案给我结果。”所以RAG这个方法并不会改变我们的模型(小孩)，而是增加了输入(书)来辅助模型生成更好的结果。\n为什么要用RAG？ #\r显而易见的，RAG可以在不改变LLM的情况下去获得好的结果，省去了finetune时耗费的时间和经历。因此，当我们想要学习一个变化很频繁的动态数据时，可以考虑使用RAG。\nRAG可以处理哪些数据？ #\rRAG不仅可以处理文本类型的数据，也可以处理PDF这种类型的数据，可谓是非常方便了。不仅如此，它还可以处理多模态相关的内容，只要集成了相关的模块即可。在这次学习中，我着目于文字相关内容。\n文件检索 #\r接下来讲讲RAG怎么进行文本检索。具体方法有基于倒排索引。\n基于倒排索引 #\r我们平时在使用的搜索引擎就是一个文本检索的过程，它是包含多个步骤的。\n简单来说，首先就是对文本进行预处理，变成规范统一的格式。比如去除标点，停用词Stop Words，字母全部变成小写等等\u0026hellip;然后就是构建文本的索引，本质就是构建倒排表。主要逻辑是记录每个词项，以及词项所在文档中间的位置信息。通过倒排表Inverted Index我们可以找到词所在的文档。\n可以在这张图片中看到这个逻辑。简单来说，词作为key，文档作为value。\n最后就是文本检索，预处理用户查询，然后计算查询中每个词项的权重。并利用检索算法(TFIDF,BM25)进行相关排序，把相关性高的文档排到前面。\nTFIDF #\rTF-IDF（Term Frequency-Inverse Document Frequency）是一种常用的文本挖掘和信息检索技术，用于衡量词汇在文档中的重要性。\n简单来说TF是词项在文档中的词频，表示一个词在文档中出现的频率。IDF是逆文档频率，表示一个词在所有文档中出现的稀有程度。\n计算公式：\n$$ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $$\n其中，𝑡是词，𝑑是文档，𝐷是文档集合。\nBM25 #\r对TFIDF的一种改进，不仅考虑到了TF，IDF。还考虑到了文档的长度doc_len。文档越长，分数越小。因为越长的文档能包含的词项肯定越多越杂，所以对问题的针对性回答肯定不会那么好，直观上比较容易理解。\n展望 #\r在实际工业中，特别是大规模数据集中，应该使用专业的文档检索工具，比如Elasticsearch。\n语义检索 #\r这一块算是NLP基础。最早的word2vec就是将词变成语义向量。BERT等模型则可以做到把句子变成语义向量。通过计算语义向量之间的相似度，可以计算出句子间，词之间的相似程度。比如apple company和iphone，如果只靠之前的文本检索，由于这两个词之间都没有相同的词，可以说是一点相似度都没有。但是我们如果从语义的角度出发，可以说两者之间有很大的关系。\n这个任务叫做mteb：Massive Text Embedding Benchmark。\n在语义检索之前，需要对文本进行文本切分，因为模型的输入是有长度上限的。\n目前在RAG中，主要是文本检索和语义检索结合使用，这样可以得到一个更高的精度。\n多路召回 #\r文本检索中增加检索精度的常用方法，通过多种召回路径进行综合，最后得到候选文档。简单来说就是通过多种检索方法进行检索，然后对结果进行加权计算得到最终的分数，这就是多路召回。\n重排序 #\r另一种增加精度的方法。具体来说就是对第一次检索出的结果，用其他更强大的模型(也叫Rerank模型)进行重排序，得到更加精确的结果。\n从复杂性和计算资源的角度来看，Rerank模型通常比初始检索模型更复杂。所以不直接使用Rerank模型，而是在粗排之后进行精排。\n举个例子，比如现在想要topk的检索结果。先用初始检索模型检索出top(k+N)个结果，然后用精排对k+N个文档重新排序，得到topk个结果。\n总结 #\rRAG作为LLM的一项应用，还是蛮有前景的，之后打算把项目摸一遍，熟悉熟悉代码。\n","date":"14 August 2024","externalUrl":null,"permalink":"/coding/llm_learning1/","section":"Codings","summary":"blog","title":"LLM RAG初步学习","type":"coding"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/life/","section":"Lives","summary":"","title":"Lives","type":"life"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/ja/categories/%E6%97%A5%E5%B8%B8/","section":"Categories","summary":"","title":"日常","type":"categories"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/categories/%E7%94%9F%E6%B4%BB/","section":"Categories","summary":"","title":"生活","type":"categories"},{"content":"\r最开始 #\r看到了一些很帅气的blog，于是自己也想搭个玩玩\n以后就有地方写coding上踩过的一些坑，以及上传自己在日本的游记了。\n想要拍照！\n","date":"10 August 2024","externalUrl":null,"permalink":"/life/test/","section":"Lives","summary":"blog","title":"突如其来地想搭建一个blog","type":"life"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/tags/%E6%9D%82%E8%B0%88/","section":"Tags","summary":"","title":"杂谈","type":"tags"},{"content":"","date":"10 August 2024","externalUrl":null,"permalink":"/ja/tags/%E9%9B%91%E8%AB%87/","section":"Tags","summary":"","title":"雑談","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]